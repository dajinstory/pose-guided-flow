{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8f2f4d04",
   "metadata": {},
   "source": [
    "# Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7da1bbd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torchvision\n",
    "# torchvision.models.vgg16(pretrained=True).features[:4].eval(),      # 64,H,W \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c176aaea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "from torchvision.transforms import InterpolationMode\n",
    "from torchvision.utils import make_grid\n",
    "from PIL import Image, ImageDraw\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from model.pgflow import PGFlowV4\n",
    "from model.landmark_detector.landmark_detector import FacialLandmarkDetector\n",
    "from model.pgflow.module import InsightFaceModule, get_header\n",
    "from util import computeGaussian, draw_edge\n",
    "\n",
    "print(torch.cuda.is_available())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "940fbb55",
   "metadata": {},
   "outputs": [],
   "source": [
    "tt = T.ToTensor()\n",
    "ptt = T.PILToTensor()\n",
    "ttp = T.ToPILImage()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "564d5d4a",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5219466",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pgflow():\n",
    "    ckpt_path = '/home/dajinhan/nas_dajinhan/experiments/pgflow/result/pgflow.ckpt'\n",
    "    pretrained = {'ckpt_path': ckpt_path}\n",
    "    net = PGFlowV4(pretrained).eval()\n",
    "    return net\n",
    "\n",
    "def load_kd_module():\n",
    "    ckpt_path = '/home/dajinhan/nas_dajinhan/models/ArcFace/model_ir_se50.pth'\n",
    "    pretrained = {'ckpt_path': ckpt_path}\n",
    "    net = InsightFaceModule(pretrained).eval()\n",
    "    return net\n",
    "\n",
    "def load_global_header():\n",
    "    ckpt_path = '/home/dajinhan/nas_dajinhan/experiments/pgflow_v4/result/global_header.ckpt'\n",
    "    net = get_header(512, 512, 32, kernel=1)\n",
    "    net.load_state_dict(torch.load(ckpt_path))\n",
    "    net = net.eval()\n",
    "    return net\n",
    "\n",
    "def load_landmark_detector():\n",
    "    ckpt_path = '/home/dajinhan/nas_dajinhan/models/landmark_detector/checkpoint/mobilefacenet_model_best.pth.tar'\n",
    "    pretrained = {'ckpt_path': ckpt_path}\n",
    "    net = FacialLandmarkDetector(pretrained).eval()\n",
    "    return net\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91e23e3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "flow = load_pgflow().cuda()\n",
    "kd_module = load_kd_module().cuda()\n",
    "global_header = load_global_header().cuda()\n",
    "landmark_detector = load_landmark_detector().cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45d0af8f",
   "metadata": {},
   "source": [
    "# Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7112ed9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_mean = [0.5, 0.5, 0.5]\n",
    "norm_std = [1.0, 1.0, 1.0]\n",
    "\n",
    "preprocess = T.Normalize(\n",
    "    mean=norm_mean, \n",
    "    std=norm_std)\n",
    "reverse_preprocess = T.Normalize(\n",
    "    mean=[-m/s for m,s in zip(norm_mean, norm_std)],\n",
    "    std=[1/s for s in norm_std])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f4a7038",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_batch(im, ldmk):\n",
    "    # Landmark Conditions\n",
    "    conditions = []\n",
    "    res = im.shape[2]\n",
    "    for _ in range(7):\n",
    "        # Computer per Image\n",
    "        heatmap = computeGaussian(ldmk, res=res, kernel_sigma=0.1, device='cuda')\n",
    "        edgemap = draw_edge(ldmk, img_size=res).cuda()\n",
    "        condition = torch.cat([heatmap, edgemap], dim=0)\n",
    "        condition = condition.unsqueeze(0)\n",
    "        conditions.append(condition)\n",
    "        res = res // 2\n",
    "\n",
    "    # Global Feature\n",
    "    kd_module.blocks.eval()\n",
    "    with torch.no_grad():\n",
    "        kd_feature = kd_module.preprocess(im) # input: norm( (0,1) )\n",
    "        for block in kd_module.blocks:\n",
    "            kd_feature = block(kd_feature)\n",
    "\n",
    "    global_feature = kd_feature.mean(dim=[2,3], keepdim=True)\n",
    "    global_feature = global_header(global_feature)\n",
    "    global_feature = torch.cat([global_feature]*im.shape[2], dim=2)\n",
    "    global_feature = torch.cat([global_feature]*im.shape[3], dim=3)\n",
    "\n",
    "    # Preprocess Inputs\n",
    "    im = preprocess(im)\n",
    "    conditions = [global_feature] + conditions[1:7]\n",
    "\n",
    "    return im, conditions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55bfc00f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_grid(x, n_row, padding=2):\n",
    "    imgs = [img.cpu() for img in x]\n",
    "    grid = make_grid(imgs, n_row, padding=padding)\n",
    "    return ttp(grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6b36c49",
   "metadata": {},
   "source": [
    "# Sample Interpolation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0eb7b71",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_image(data, filename):\n",
    "    fig = plt.figure(figsize=(1, 1))\n",
    "    ax = plt.Axes(fig, [0., 0., 1., 1.])\n",
    "    ax.set_axis_off()\n",
    "    fig.add_axes(ax)\n",
    "    ax.imshow(data)\n",
    "    fig.savefig(filename, dpi=data.shape[0]) \n",
    "    plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "301a3493",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_image(conditions_s[1][0][-1].cpu(), 'posemap5.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19518663",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.imshow(conditions_s[1][0][-1].cpu())\n",
    "plt.axis('off')\n",
    "plt.margins(0, 0)\n",
    "plt.gca().xaxis.set_major_locator(plt.NullLocator())\n",
    "plt.gca().yaxis.set_major_locator(plt.NullLocator())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0322fd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "im_s = ptt(Image.open('/home/dajinhan/nas_dajinhan/datasets/CelebAHQ/resized64x64/00002.png')).reshape(1,3,64,64).cuda()\n",
    "im_t = ptt(Image.open('/home/dajinhan/nas_dajinhan/datasets/CelebAHQ/resized64x64/00009.png')).reshape(1,3,64,64).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96d125ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_grid(torch.cat([im_s, im_t], dim=0), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecf2c17a",
   "metadata": {},
   "outputs": [],
   "source": [
    "im_s_resized = T.Resize(112, interpolation=InterpolationMode.BICUBIC, antialias=True)(im_s)\n",
    "im_t_resized = T.Resize(112, interpolation=InterpolationMode.BICUBIC, antialias=True)(im_t)\n",
    "\n",
    "ldmk_s, f5p_s = landmark_detector(im_s_resized) # input: (0,1)\n",
    "ldmk_t, f5p_t = landmark_detector(im_t_resized) # input: (0,1)\n",
    "\n",
    "w_s, _, _, _, _ = flow.forward(*preprocess_batch(im_s, ldmk_s[0]))\n",
    "w_t, _, _, _, _ = flow.forward(*preprocess_batch(im_t, ldmk_t[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdf9d865",
   "metadata": {},
   "outputs": [],
   "source": [
    "def new_draw_edge(ldmk, img_size=112):\n",
    "        n_partials = [17, 5, 5, 4, 5, 6, 6, 12, 8] # uface, lbrow, rbrow, hnose, wnose, leye, reye, mouth_out, mouth_in\n",
    "        img  = Image.new( mode = \"L\", size = (img_size, img_size) )\n",
    "        draw = ImageDraw.Draw(img)\n",
    "\n",
    "        idx=0\n",
    "        for n_partial in n_partials:\n",
    "            x_s, y_s = torch.floor(ldmk[idx] * img_size)\n",
    "            for x_e, y_e in ldmk[idx+1:idx+n_partial]:\n",
    "                x_e = torch.floor(x_e * img_size)\n",
    "                y_e = torch.floor(y_e * img_size)\n",
    "                draw.line((x_s, y_s, x_e, y_e), fill=255)\n",
    "                x_s, y_s = x_e, y_e\n",
    "            idx += n_partial\n",
    "\n",
    "        return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3951888",
   "metadata": {},
   "outputs": [],
   "source": [
    "edgemap = new_draw_edge(ldmk_s[0], img_size=64)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b4ea87e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ptt(edgemap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd50f56b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tt(edgemap).type()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c7ebff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "edgemap.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79cd7235",
   "metadata": {},
   "outputs": [],
   "source": [
    "T.ToTensor()(edgemap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad1c1607",
   "metadata": {},
   "outputs": [],
   "source": [
    "(edgemap/255).cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12bb677c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ttp(edgemap/255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c38f503c",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_frames = 2\n",
    "w_list = [ (w_t*frame_idx + w_s*(n_frames-frame_idx)) / n_frames for frame_idx in range(n_frames+1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78289283",
   "metadata": {},
   "outputs": [],
   "source": [
    "frames = []\n",
    "im_s, conditions_s = preprocess_batch(im_s, ldmk_s[0])\n",
    "\n",
    "with torch.no_grad():\n",
    "    # Preprocess\n",
    "    im_s, conditions_s = preprocess_batch(im_s, ldmk_s[0])\n",
    "    im_t, conditions_t = preprocess_batch(im_t, ldmk_t[0])\n",
    "\n",
    "    # Forward: x->z\n",
    "    w_s, log_p_s, log_det_s, splits_s, inter_features_s = flow.forward(im_s, conditions_s)\n",
    "    \n",
    "    for w in w_list:    \n",
    "        splits = [torch.zeros_like(split) if split is not None else split for split in splits_s]\n",
    "\n",
    "        # Reverse: z->x\n",
    "        im_rec = flow.reverse(w, conditions_s, splits)\n",
    "#         im_rec = flow.reverse(w_t, conditions_s, splits)\n",
    "        im_rec = reverse_preprocess(im_rec)\n",
    "        im_rec = torch.clamp(im_rec, 0, 1)\n",
    "\n",
    "        # Update\n",
    "        frames.append(im_rec)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7916d13",
   "metadata": {},
   "outputs": [],
   "source": [
    "ttp(conditions_s[1][0][68]/255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88b231d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "conditions_s[1][0][68].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dafd000",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_grid(torch.cat(frames, dim=0), 5, padding=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21618ac6",
   "metadata": {},
   "source": [
    "# Sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54be0f36",
   "metadata": {},
   "outputs": [],
   "source": [
    "im_s = ptt(Image.open('/home/dajinhan/nas_dajinhan/datasets/CelebAHQ/resized64x64/00002.png')).reshape(1,3,64,64).cuda()\n",
    "im_t = ptt(Image.open('/home/dajinhan/nas_dajinhan/datasets/CelebAHQ/resized64x64/00009.png')).reshape(1,3,64,64).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0479549e",
   "metadata": {},
   "outputs": [],
   "source": [
    "im_s = ptt(Image.open('/data/dajinhan/datasets/VoxCeleb/unzippedFaces_resized/64x64_png/Kellan_Lutz/1.6/482zKSvHeRw/0003350.png')).reshape(1,3,64,64).cuda()\n",
    "im_t = ptt(Image.open('/data/dajinhan/datasets/VoxCeleb/unzippedFaces_resized/64x64_png/Kellan_Lutz/1.6/482zKSvHeRw/0003400.png')).reshape(1,3,64,64).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e670cae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_grid(torch.cat([im_s, im_t], dim=0), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22a32850",
   "metadata": {},
   "outputs": [],
   "source": [
    "im_s_resized = T.Resize(112, interpolation=InterpolationMode.BICUBIC, antialias=True)(im_s)\n",
    "im_t_resized = T.Resize(112, interpolation=InterpolationMode.BICUBIC, antialias=True)(im_t)\n",
    "\n",
    "ldmk_s, f5p_s = landmark_detector(im_s_resized) # input: (0,1)\n",
    "ldmk_t, f5p_t = landmark_detector(im_t_resized) # input: (0,1)\n",
    "\n",
    "w_s, _, _, _, _ = flow.forward(*preprocess_batch(im_s, ldmk_s[0]))\n",
    "w_t, _, _, _, _ = flow.forward(*preprocess_batch(im_t, ldmk_t[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26e5c4d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_frames = 4\n",
    "landmarks = [ (ldmk_t*frame_idx + ldmk_s*(n_frames-frame_idx)) / n_frames for frame_idx in range(n_frames+1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "627686b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "frames = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    im = im_s\n",
    "    ldmk = ldmk_s\n",
    "    for idx, ldmk_next in enumerate(landmarks):\n",
    "        # Preprocess\n",
    "        # im, conditions = preprocess_batch(im, ldmk[0])\n",
    "        im, conditions = preprocess_batch(im_s, ldmk_s[0])\n",
    "        _, conditions_next = preprocess_batch(im, ldmk_next[0])\n",
    "\n",
    "        # Forward: x->z\n",
    "        w, log_p, log_det, splits, inter_features = flow.forward(im, conditions)\n",
    "        splits = [torch.zeros_like(split) if split is not None else split for split in splits]\n",
    "\n",
    "        # Reverse: z->x\n",
    "#         im_rec = flow.reverse(w, conditions_next, splits)\n",
    "        im_rec = flow.reverse(0.3*torch.randn_like(w), conditions_next, splits)\n",
    "        # im_rec = flow.reverse((w_s+w_t)/2, conditions_next, splits)\n",
    "        im_rec = reverse_preprocess(im_rec)\n",
    "        im_rec = torch.clamp(im_rec, 0, 1)\n",
    "\n",
    "        # Update\n",
    "        im = im_rec\n",
    "        ldmk = ldmk_next\n",
    "        frames.append(im)\n",
    "        \n",
    "        # Save image\n",
    "        # im_PIL = ttp(im[0])\n",
    "        # im_PIL.save('%d.png'%(idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e87d320",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_grid(torch.cat(frames, dim=0), n_frames+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da759fc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_grid(torch.cat(frames, dim=0), n_frames+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e02012b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "frames = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    im = im_s\n",
    "    ldmk = ldmk_s\n",
    "    for idx, ldmk_next in enumerate(landmarks):\n",
    "        # Preprocess\n",
    "        # im, conditions = preprocess_batch(im, ldmk[0])\n",
    "        im, conditions = preprocess_batch(im_s, ldmk_s[0])\n",
    "        _, conditions_next = preprocess_batch(im, ldmk_next[0])\n",
    "\n",
    "        # Forward: x->z\n",
    "        w, log_p, log_det, splits, inter_features = flow.forward(im, conditions)\n",
    "#         splits = [torch.zeros_like(split) if split is not None else split for split in splits]\n",
    "\n",
    "        # Reverse: z->x\n",
    "        im_rec = flow.reverse(w, conditions_next, splits)\n",
    "        # im_rec = flow.reverse(0.3*torch.randn_like(w), conditions_next, splits)\n",
    "        # im_rec = flow.reverse((w_s+w_t)/2, conditions_next, splits)\n",
    "        im_rec = reverse_preprocess(im_rec)\n",
    "        im_rec = torch.clamp(im_rec, 0, 1)\n",
    "\n",
    "        # Update\n",
    "        im = im_rec\n",
    "        ldmk = ldmk_next\n",
    "        frames.append(im)\n",
    "        \n",
    "        # Save image\n",
    "        # im_PIL = ttp(im[0])\n",
    "        # im_PIL.save('%d.png'%(idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee3c6904",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_grid(torch.cat(frames, dim=0), n_frames+1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "030d02c8",
   "metadata": {},
   "source": [
    "# Sample Figure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4908876",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_figure(im_s, im_t):\n",
    "    # Get LDMK\n",
    "    im_s_resized = T.Resize(112, interpolation=InterpolationMode.BICUBIC, antialias=True)(im_s)\n",
    "    im_t_resized = T.Resize(112, interpolation=InterpolationMode.BICUBIC, antialias=True)(im_t)\n",
    "    ldmk_s, f5p_s = landmark_detector(im_s_resized) # input: (0,1)\n",
    "    ldmk_t, f5p_t = landmark_detector(im_t_resized) # input: (0,1)\n",
    "\n",
    "    # Preprocess\n",
    "    im_s, conditions_s = preprocess_batch(im_s, ldmk_s[0])\n",
    "    im_t, conditions_t = preprocess_batch(im_t, ldmk_t[0])\n",
    "\n",
    "    # Forward: x->z\n",
    "    w, log_p, log_det, splits, inter_features = flow.forward(im_s, conditions_s)\n",
    "    splits = [torch.zeros_like(split) if split is not None else split for split in splits]\n",
    "\n",
    "    # Reverse: z->x\n",
    "    im_rec = flow.reverse(w, conditions_t, splits)\n",
    "    # im_rec = flow.reverse(0.3*torch.randn_like(w), conditions_next, splits)\n",
    "    # im_rec = flow.reverse((w_s+w_t)/2, conditions_next, splits)\n",
    "    im_rec = reverse_preprocess(im_rec)\n",
    "    im_rec = torch.clamp(im_rec, 0, 1)\n",
    "    \n",
    "    return im_rec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a84732f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "im_s = ptt(Image.open('/data/dajinhan/datasets/VoxCeleb/unzippedFaces_resized/64x64_png/Kellan_Lutz/1.6/5TnxCj72FLg/0000850.png')).reshape(1,3,64,64).cuda()\n",
    "im_t = ptt(Image.open('/data/dajinhan/datasets/VoxCeleb/unzippedFaces_resized/64x64_png/Kellan_Lutz/1.6/5TnxCj72FLg/0000925.png')).reshape(1,3,64,64).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44ec993e",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08c01865",
   "metadata": {},
   "outputs": [],
   "source": [
    "'%.3d'%(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bc569c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "gens = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cc47e2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "Image.open('/home/dajinhan/nas_dajinhan/datasets/CelebAHQ/resized64x64/00003.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da064d45",
   "metadata": {},
   "outputs": [],
   "source": [
    "gens = []\n",
    "gens.append(torch.zeros((1,3,64,64)).cuda())\n",
    "for t_idx in [31,32,34,3,4]:\n",
    "    im_t = ptt(Image.open('/home/dajinhan/nas_dajinhan/datasets/CelebAHQ/resized64x64/%.5d.png'%(t_idx))).reshape(1,3,64,64).cuda()\n",
    "    gens.append(im_t)\n",
    "for s_idx in  [1,2,4,6,9]:\n",
    "    im_s = ptt(Image.open('/home/dajinhan/nas_dajinhan/datasets/CelebAHQ/resized64x64/%.5d.png'%(s_idx))).reshape(1,3,64,64).cuda()\n",
    "    gens.append(im_s)\n",
    "    for t_idx in [31,32,34,3,4]:\n",
    "        im_t = ptt(Image.open('/home/dajinhan/nas_dajinhan/datasets/CelebAHQ/resized64x64/%.5d.png'%(t_idx))).reshape(1,3,64,64).cuda()\n",
    "        gens.append(sample_figure(im_s, im_t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05ce9c37",
   "metadata": {},
   "outputs": [],
   "source": [
    "gens[0] = torch.ones_like(gens[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccdf9f93",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_grid(torch.cat(gens, dim=0), 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35ac8340",
   "metadata": {},
   "outputs": [],
   "source": [
    "gens2 = [F.pad(gg, (1,1), mode='constant', value=1) for gg in gens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dae3496",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_grid(torch.cat(gens2, dim=0), 6, padding=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "760b30e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "im_rec = sample_figure(im_s, im_t)\n",
    "sample_grid(torch.cat([im_s, im_rec, im_t], dim=0), 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddc4de78",
   "metadata": {},
   "outputs": [],
   "source": [
    "im_s = ptt(Image.open('/data/dajinhan/datasets/VoxCeleb/unzippedFaces_resized/64x64_png/Kristen_Stewart/1.6/nU6gCC3DTCc/0004900.png')).reshape(1,3,64,64).cuda()\n",
    "im_t = ptt(Image.open('/data/dajinhan/datasets/VoxCeleb/unzippedFaces_resized/64x64_png/Kristen_Stewart/1.6/nU6gCC3DTCc/0003400.png')).reshape(1,3,64,64).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2577960c",
   "metadata": {},
   "outputs": [],
   "source": [
    "im_s = ptt(Image.open('/data/dajinhan/datasets/VoxCeleb/unzippedFaces_resized/64x64_png/Kristen_Stewart/1.6/JTCz4B9pwT4/0000800.png')).reshape(1,3,64,64).cuda()\n",
    "im_t = ptt(Image.open('/data/dajinhan/datasets/VoxCeleb/unzippedFaces_resized/64x64_png/Kristen_Stewart/1.6/JTCz4B9pwT4/0001000.png')).reshape(1,3,64,64).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b1afe12",
   "metadata": {},
   "outputs": [],
   "source": [
    "ttp(im_s[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f92cd9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_figure(im_s, im_t):\n",
    "    # Get LDMK\n",
    "    im_s_resized = T.Resize(112, interpolation=InterpolationMode.BICUBIC, antialias=True)(im_s)\n",
    "    im_t_resized = T.Resize(112, interpolation=InterpolationMode.BICUBIC, antialias=True)(im_t)\n",
    "    ldmk_s, f5p_s = landmark_detector(im_s_resized) # input: (0,1)\n",
    "    ldmk_t, f5p_t = landmark_detector(im_t_resized) # input: (0,1)\n",
    "\n",
    "    # Preprocess\n",
    "    im_s, conditions_s = preprocess_batch(im_s, ldmk_s[0])\n",
    "    im_t, conditions_t = preprocess_batch(im_t, ldmk_t[0])\n",
    "\n",
    "    # Forward: x->z\n",
    "    w, log_p, log_det, splits, inter_features = flow.forward(im_s, conditions)\n",
    "    splits = [torch.zeros_like(split) if split is not None else split for split in splits]\n",
    "\n",
    "    # Reverse: z->x\n",
    "    im_rec = flow.reverse(w, conditions_t, splits)\n",
    "    # im_rec = flow.reverse(0.3*torch.randn_like(w), conditions_next, splits)\n",
    "    # im_rec = flow.reverse((w_s+w_t)/2, conditions_next, splits)\n",
    "    im_rec = reverse_preprocess(im_rec)\n",
    "    im_rec = torch.clamp(im_rec, 0, 1)\n",
    "    \n",
    "    return im_rec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d087f585",
   "metadata": {},
   "outputs": [],
   "source": [
    "im_s2 = ptt(Image.open('/data/dajinhan/datasets/VoxCeleb/unzippedFaces_resized/64x64_png/Kristen_Stewart/1.6/nU6gCC3DTCc/0002150.png')).reshape(1,3,64,64).cuda()\n",
    "im_t2 = ptt(Image.open('/data/dajinhan/datasets/VoxCeleb/unzippedFaces_resized/64x64_png/Kristen_Stewart/1.6/nU6gCC3DTCc/0001925.png')).reshape(1,3,64,64).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1963d842",
   "metadata": {},
   "outputs": [],
   "source": [
    "im_rec2 = sample_figure(im_s2, im_t2)\n",
    "sample_grid(torch.cat([im_s2, im_rec2, im_t2], dim=0), 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33ad9608",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_grid(torch.cat([im_s, im_rec, im_t, im_s2, im_rec2, im_t2], dim=0), 3).save('figure.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "077d0734",
   "metadata": {},
   "outputs": [],
   "source": [
    "im, conditions = preprocess_batch(im_s, ldmk_s[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mmedit",
   "language": "python",
   "name": "mmedit"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
